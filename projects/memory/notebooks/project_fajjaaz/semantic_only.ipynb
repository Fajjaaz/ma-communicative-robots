{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fcff364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "\n",
    "def read_json(path):\n",
    "    \"\"\"Read json file.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    path: path to the json\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loaded: loaded dict\n",
    "\n",
    "    \"\"\"\n",
    "    with open(path, \"r\") as stream:\n",
    "        loaded = json.load(stream)\n",
    "\n",
    "    return loaded\n",
    "\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "\n",
    "def natural_keys(text):\n",
    "    \"\"\"\n",
    "    copied from https://stackoverflow.com/questions/5967500/how-to-correctly-sort-a-string-with-a-number-inside\n",
    "    alist.sort(key=natural_keys) sorts in human order\n",
    "    http://nedbatchelder.com/blog/200712/human_sorting.html\n",
    "    (See Toothy's implementation in the comments)\n",
    "\n",
    "    \"\"\"\n",
    "    return [atoi(c) for c in re.split(r\"(\\d+)\", text)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e65ee2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a2f137e2b6a4aab97e41ef2c04840eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data\\128_1.json \t ['val', 'test', 'max_history', 'capacity', 'rewards', 'accuracy']\n",
      "maximum_history: 128\n",
      "memory_capacity per system: 1\n",
      "\n",
      "../data\\128_2.json \t ['val', 'test', 'max_history', 'capacity', 'rewards', 'accuracy']\n",
      "maximum_history: 128\n",
      "memory_capacity per system: 2\n",
      "\n",
      "../data\\128_4.json \t ['val', 'test', 'max_history', 'capacity', 'rewards', 'accuracy']\n",
      "maximum_history: 128\n",
      "memory_capacity per system: 4\n",
      "\n",
      "../data\\128_8.json \t ['val', 'test', 'max_history', 'capacity', 'rewards', 'accuracy']\n",
      "maximum_history: 128\n",
      "memory_capacity per system: 8\n",
      "\n",
      "../data\\128_16.json \t ['val', 'test', 'max_history', 'capacity', 'rewards', 'accuracy']\n",
      "maximum_history: 128\n",
      "memory_capacity per system: 16\n",
      "\n",
      "../data\\128_32.json \t ['val', 'test', 'max_history', 'capacity', 'rewards', 'accuracy']\n",
      "maximum_history: 128\n",
      "memory_capacity per system: 32\n",
      "\n",
      "../data\\128_64.json \t ['val', 'test', 'max_history', 'capacity', 'rewards', 'accuracy']\n",
      "maximum_history: 128\n",
      "memory_capacity per system: 64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_all = {}\n",
    "\n",
    "paths = glob(\"../data/*.json\")\n",
    "paths.sort(key=natural_keys)\n",
    "for path in tqdm(paths):\n",
    "    data = read_json(path)\n",
    "\n",
    "    print(path, \"\\t\", list(data.keys()))\n",
    "    data_all[path] = data\n",
    "\n",
    "    maximum_history = os.path.basename(path).split(\"_\")[0]\n",
    "    memory_capacity = os.path.basename(path).split(\"_\")[1].split(\".json\")[0]\n",
    "\n",
    "    print(\n",
    "        f\"maximum_history: {maximum_history}\\nmemory_capacity per system: {memory_capacity}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d81e486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"tscholak/t5.1.1.lm100k.base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"tscholak/t5.1.1.lm100k.base\")\n",
    "\n",
    "\n",
    "# We want to do this but this is too big. Tae'll run your prompts on a beefy server later.\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bigscience/T0pp\")\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0pp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8a5cee",
   "metadata": {},
   "source": [
    "## Prompts using semantic memories only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9c66c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "k = 1\n",
    "scores = []\n",
    "while j <= 6:\n",
    "    prompts = []\n",
    "    answers = []\n",
    "    for i in range(len(data_all[\"../data\\\\128_\"+str(k)+\".json\"][\"test\"])):\n",
    "        random_point = data_all[\"../data\\\\128_\"+str(k)+\".json\"][\"test\"][i]\n",
    "        random_point[\"episodic_memory_system\"] = sorted(random_point[\"episodic_memory_system\"], key=lambda x: x[-1])\n",
    "        for idx, mem in enumerate(random_point['episodic_memory_system']):\n",
    "            max_len = len(random_point[\"episodic_memory_system\"])\n",
    "            days = len(random_point['episodic_memory_system']) - idx - 1\n",
    "            if days == 0:\n",
    "                timestamp = \"today\"\n",
    "            else:\n",
    "                timestamp = f\"{days} days ago\"\n",
    "            random_point['episodic_memory_system'][idx][-1] = timestamp\n",
    "\n",
    "        prompt = []\n",
    "\n",
    "        for mem in random_point[\"semantic_memory_system\"]:\n",
    "            prompt.append(f\"{mem[-1]} {mem[0]} were found at {mem[2]}.\")\n",
    "        prompt.append(f\"Where is {random_point['question'][0]}?\")\n",
    "        prompt = ' '.join(prompt)\n",
    "\n",
    "        prompts.append(prompt)\n",
    "        answers.append(random_point['correct_answer'])\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "    for prompt in prompts:\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "        outputs = model.generate(input_ids)\n",
    "        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        predictions.append(prediction)\n",
    "        \n",
    "    correct = 0\n",
    "    wrong = 0\n",
    "    for answer, pred in zip(answers, predictions):\n",
    "        if answer in pred:\n",
    "            correct += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "    scores.append(correct / (correct+ wrong))\n",
    "    \n",
    "    j+=1\n",
    "    k*=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8622e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128_1.json test accuracy: 0.0390625\n",
      "128_2.json test accuracy: 0.0625\n",
      "128_4.json test accuracy: 0.0234375\n",
      "128_8.json test accuracy: 0.0859375\n",
      "128_16.json test accuracy: 0.09375\n",
      "128_32.json test accuracy: 0.09375\n",
      "128_64.json test accuracy: 0.0390625\n"
     ]
    }
   ],
   "source": [
    "j=0\n",
    "k=1\n",
    "while j<=6:\n",
    "    print(\"128_\"+str(k)+\".json test accuracy: \"+str(scores[j]))\n",
    "    j+=1\n",
    "    k*=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e890a684",
   "metadata": {},
   "source": [
    "## converted all episodic to semantic and added semantic memories to the prompt based on the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8bb1b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0f0a8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (526 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "j = 0\n",
    "k = 1\n",
    "scores = []\n",
    "while j <= 6:\n",
    "    cols = [\"object\", \"location\"]\n",
    "    lss = []\n",
    "\n",
    "    for i in range(len(data_all[\"../data\\\\128_\"+str(k)+\".json\"][\"test\"])):\n",
    "        random_point = data_all[\"../data\\\\128_\"+str(k)+\".json\"][\"test\"][i]\n",
    "        random_point[\"episodic_memory_system\"] = sorted(random_point[\"episodic_memory_system\"], key=lambda x: x[-1])\n",
    "\n",
    "        for mem in random_point[\"episodic_memory_system\"]:\n",
    "            lss.append([\"\".join(mem[0].split()[1:2]), \"\".join(mem[2].split()[1:2])])\n",
    "    observations = pd.DataFrame(lss, columns = cols)\n",
    "    \n",
    "    obs = observations.groupby(observations.columns.tolist()).size().reset_index().rename(columns={0:'ranking'})\n",
    "    obs_rank_sorted = obs.sort_values('ranking', ascending = False).reset_index(drop=True)\n",
    "    \n",
    "    prompts = []\n",
    "    answers = []\n",
    "    for i in range(len(data_all[\"../data\\\\128_\"+str(k)+\".json\"][\"test\"])):\n",
    "        random_point = data_all[\"../data\\\\128_\"+str(k)+\".json\"][\"test\"][i]\n",
    "        for idx, mem in enumerate(random_point['episodic_memory_system']):\n",
    "            max_len = len(random_point[\"episodic_memory_system\"])\n",
    "            days = len(random_point['episodic_memory_system']) - idx - 1\n",
    "            if days == 0:\n",
    "                timestamp = \"today\"\n",
    "            else:\n",
    "                timestamp = f\"{days} days ago\"\n",
    "            random_point['episodic_memory_system'][idx][-1] = timestamp\n",
    "\n",
    "        prompt = []\n",
    "        episodic_object = obs_rank_sorted.loc[obs_rank_sorted['object'] == \"\".join(random_point['question'][0].split()[1:2])].reset_index()\n",
    "\n",
    "        for mem in random_point[\"episodic_memory_system\"]:\n",
    "            prompt.append(f\"{mem[0]} was at {mem[2]}, {mem[3]}.\")\n",
    "\n",
    "        if len(episodic_object) >= k:\n",
    "            for i in range(k):\n",
    "                prompt.append(str(episodic_object[\"ranking\"][i]) +\" \" +str(episodic_object[\"object\"][i]) + \" were found at \"+str(episodic_object[\"location\"][i])+\".\")\n",
    "        else:\n",
    "            for i in range(len(episodic_object)):\n",
    "                prompt.append(str(episodic_object[\"ranking\"][i]) +\" \" +str(episodic_object[\"object\"][i]) + \" were found at \"+str(episodic_object[\"location\"][i])+\".\")\n",
    "\n",
    "        prompt.append(f\"Where is {random_point['question'][0]}?\")\n",
    "        prompt = ' '.join(prompt)\n",
    "\n",
    "        prompts.append(prompt)\n",
    "        answers.append(random_point['correct_answer'])\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "    for prompt in prompts:\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "        outputs = model.generate(input_ids)\n",
    "        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        predictions.append(prediction)\n",
    "        \n",
    "    correct = 0\n",
    "    wrong = 0\n",
    "    for answer, pred in zip(answers, predictions):\n",
    "        if answer in pred:\n",
    "            correct += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "            \n",
    "    scores.append(correct / (correct+ wrong))\n",
    "    j+=1\n",
    "    k*=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee3c52f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128_1.json test accuracy: 0.5703125\n",
      "128_2.json test accuracy: 0.3671875\n",
      "128_4.json test accuracy: 0.375\n",
      "128_8.json test accuracy: 0.359375\n",
      "128_16.json test accuracy: 0.421875\n",
      "128_32.json test accuracy: 0.4453125\n",
      "128_64.json test accuracy: 0.5546875\n"
     ]
    }
   ],
   "source": [
    "j=0\n",
    "k=1\n",
    "while j<=6:\n",
    "    print(\"128_\"+str(k)+\".json test accuracy: \"+str(scores[j]))\n",
    "    j+=1\n",
    "    k*=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e909718",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
